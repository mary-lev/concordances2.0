#!/usr/bin/env python
# coding: utf8

import nltk
import pymorphy2
from operator import itemgetter

def tokenize1(text1): # tokenization of decoded text, returns list of tokens
    text3 = nltk.wordpunct_tokenize(text1)
    stop = [',', '.', ';', ':', '!', '?', '!"', '..', '!.', '...', '—', '-', '«', '»', '»', '"', '— ', '—', '!».', '!»', '4', '5', '6', '7', '8', '9', '0', " "]
    tokens = [s.lower() for s in text3 if s not in stop]
    return tokens

def tokenize2(text1): # tokenization of decoded text, returns list of tokens, for def_create_xml
    text3 = nltk.wordpunct_tokenize(text1)
    return text3

def normalize_new2(text1, n):
    text3 = nltk.wordpunct_tokenize(text1)
    morph = pymorphy2.MorphAnalyzer()
    normal = []
    count=0
    for w in text3:
        p = morph.parse(w)[n]
        normal.append([p.normal_form, p.tag.POS, w, count, p.tag.tense])
        count +=1
    else:
        pass
    return normal

def normalize_new3(text1, n): # return only lemma
    text3 = nltk.wordpunct_tokenize(text1)
    morph = pymorphy2.MorphAnalyzer()
    normal = []
    for w in text3:
        p = morph.parse(w)[n]
        normal.append(p.normal_form)
    else:
        pass
    return normal
